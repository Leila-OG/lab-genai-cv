# lab1-genai-cv

## Partie 1 : ImplÃ©mentation du Variational Autoencoder (VAE)

### RÃ©ponses aux Questions
**1. Pourquoi utilise-t-on le reparameterization trick dans les VAE ?**

Le reparameterization trick (z=Î¼+Ïƒâ‹…Ïµ) permet de rendre le processus dâ€™Ã©chantillonnage diffÃ©rentiable vis-Ã -vis des paramÃ¨tres du rÃ©seau (mean et log_var). Sans ce trick, lâ€™Ã©chantillonnage alÃ©atoire empÃªcherait le calcul du gradient Ã  travers la couche stochastique. En sÃ©parant la partie alÃ©atoire (Ïµâˆ¼N(0,1)) de la partie dÃ©terministe (calcul de Î¼ et Ïƒ), on permet la rÃ©tropropagation standard.

**2. Comment la perte de divergence KL (KL Divergence) affecte-t-elle lâ€™espace latent ?**

La divergence KL agit comme un terme de rÃ©gularisation. Elle pousse la distribution encodÃ©e (dÃ©finie par Î¼ et Ïƒ) Ã  se rapprocher dâ€™une distribution gaussienne standard N(0,I). En pratique, cela Ã©vite que lâ€™encodeur nâ€™exploite Â« trop Â» de libertÃ© dans lâ€™espace latent ; il tend Ã  Â« recentrer Â» et Â« resserrer Â» les Ã©chantillons autour de 0, imposant une structure plus rÃ©guliÃ¨re et plus lisse dans lâ€™espace latent.

**3. Comment la modification de la dimension de lâ€™espace latent (latent_dim) impacte-t-elle la qualitÃ© de la reconstruction ?**

En gÃ©nÃ©ral, plus latent_dim est grand, plus le modÃ¨le dispose de capacitÃ©s pour encoder lâ€™information, ce qui peut amÃ©liorer la qualitÃ© de reconstruction (au risque dâ€™overfitter).
Moins latent_dim est grand, plus la reprÃ©sentation est contrainte et donc moins prÃ©cise, ce qui peut se traduire par des reconstructions de moins bonne qualitÃ© mais un espace latent plus Â« compact Â» et potentiellement plus rÃ©gulier pour la gÃ©nÃ©ration.
Il existe donc un compromis Ã  trouver en fonction de la tÃ¢che (qualitÃ© de reconstruction, gÃ©nÃ©ration, etc.).


## Partie 1 : De VAE Ã  VAE

### RÃ©ponses aux Questions
**1. Comment le dÃ©codeur dâ€™un VAE peut-il servir de gÃ©nÃ©rateur pour un GAN ?**

Dans un GAN (Generative Adversarial Network), le gÃ©nÃ©rateur prend un Ã©chantillon 
ğ‘§
z (gÃ©nÃ©ralement issu dâ€™une distribution normale N(0,I)) et produit une image (ou un signal) rÃ©aliste.
Le dÃ©codeur dâ€™un VAE fait exactement cela : Ã©tant donnÃ© un vecteur latent
**z**, il gÃ©nÃ¨re une image. Ainsi, on peut rÃ©utiliser la mÃªme architecture (et mÃªme les poids si lâ€™on souhaite) du dÃ©codeur pour servir de gÃ©nÃ©rateur dans un GAN.

**2. DiffÃ©rences entre lâ€™encodeur VAE et le discriminateur GAN**

Un encodeur de VAE prend une image et la projette dans un espace latent (retourne Î¼ et logÏƒ^2). Il ne produit pas une probabilitÃ© Â« vrai/faux Â», mais des paramÃ¨tres de distribution latente.
Un discriminateur de GAN prend une image (rÃ©elle ou gÃ©nÃ©rÃ©e) et retourne une probabilitÃ© quâ€™elle soit rÃ©elle (vs. fake). Il sâ€™agit dâ€™un classifieur binaire, tandis que lâ€™encodeur sert plutÃ´t dâ€™Â« encodeur probabiliste Â».

## RÃ©sultats 

### Test 1
![alt text](image-1.png)
![alt text](image.png)